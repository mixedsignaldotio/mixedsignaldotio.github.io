<!doctype html>
<html lang="en">
<head>
  <meta charset="utf-8" />
  <title>Scaling Neuromorphic Systems</title>
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <link rel="stylesheet" href="../../assets/styles.css" />
  <script defer src="../../assets/script.js"></script>
  <link rel="icon" type="image/x-icon" href="../../assets/images/favicon_xf_eye_fancy_2_maxsize.ico" sizes="any">
  <!-- GOOGLE AD BEGIN -->
  <script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-XXXXXXXXXXXXXXX" crossorigin="anonymous"></script>
  <!-- GOOGLE AD END -->
  <!-- Google Font for brand title only -->
  <link rel="preconnect" href="https://fonts.googleapis.com">
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
  <link href="https://fonts.googleapis.com/css2?family=Bodoni+Moda:ital,opsz,wght@0,6..96,400..900;1,6..96,400..900&display=swap" rel="stylesheet">
  <!-- MathJax (v3) -->
  <script>
    window.MathJax = {
      tex: { inlineMath: [['$', '$'], ['\\(', '\\)']] },
      svg: { fontCache: 'global' }
    };
  </script>
  <script defer src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-svg.js"></script>
</head>
<body>

  <!-- Sticky header (mobile/tablet active; desktop shows floating tools via CSS) -->
  <header id="sticky-header" aria-hidden="true">
    <div class="header-inner">
      <div class="article-title">Scaling Neuromorphic Systems</div>
      <div class="header-actions">
        <div class="toc-dropdown">
          <button id="toc-button" aria-haspopup="true" aria-expanded="false">Navigation â–¾</button>
          <nav id="toc-menu" class="toc-menu" aria-label="Table of contents"></nav>
        </div>
        <button id="theme-toggle" title="Toggle theme" aria-label="Toggle theme">ðŸŒ“</button>
      </div>
    </div>
  </header>

  <!-- ARTICLE CONTENT ONLY -->
  <main id="article">
    <!-- OLD STUFF
    <img class="article-logo logo-light" src="../../assets/images/hackframe_header_light.svg" alt="Hackframe logo (light)" />
    <img class="article-logo logo-dark"  src="../../assets/images/hackframe_header_dark.svg"  alt="Hackframe logo (dark)" /> 
    -->

    <!-- NEW STUFF: USE THIS : Brand row: logo (left) + optional buttons (right) -->
     <div class="article-brand-row">
       <div class="brand-logo">
         <!-- <img class="article-logo logo-light" src="../../assets/images/hackframe_header_light.svg" alt="Hackframe logo (light)" />
         <img class="article-logo logo-dark"  src="../../assets/images/hackframe_header_dark.svg"  alt="Hackframe logo (dark)" /> -->
       </div>
       <nav class="brand-actions" aria-label="Article quick actions">
         <!-- <a class="btn-icon" href="https://github.com/your/repo" target="_blank" rel="noopener" title="GitHub">
           <svg viewBox="0 0 24 24" aria-hidden="true" fill="currentColor"><path d="M12 .5a12 12 0 0 0-3.79 23.4c.6.11.82-.26.82-.58v-2.02c-3.34.73-4.04-1.61-4.04-1.61-.55-1.41-1.34-1.78-1.34-1.78-1.09-.74.08-.72.08-.72 1.2.08 1.83 1.23 1.83 1.23 1.07 1.83 2.8 1.3 3.48.99.11-.78.42-1.3.77-1.6-2.67-.3-5.48-1.34-5.48-5.97 0-1.32.47-2.39 1.23-3.24-.12-.3-.53-1.52.12-3.17 0 0 1.01-.32 3.3 1.23a11.46 11.46 0 0 1 6 0c2.29-1.55 3.3-1.23 3.3-1.23.65 1.65.24 2.87.12 3.17.77.85 1.23 1.92 1.23 3.24 0 4.64-2.81 5.66-5.49 5.96.43.38.81 1.12.81 2.26v3.35c0 .32.22.7.83.58A12 12 0 0 0 12 .5z"/></svg>
         </a> -->
         <!-- <a class="btn-icon" href="paper.pdf" target="_blank" rel="noopener" title="Download PDF">
           <svg viewBox="0 0 24 24" aria-hidden="true" fill="currentColor"><path d="M14 2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2h12a2 2 0 0 0 2-2V8zm0 2.5L18.5 9H14zM7 13h2a2 2 0 0 0 0-4H7v4zm1.5-2.5H9a.5.5 0 0 1 0 1H8.5zm3.5 2.5h1.25c1.1 0 2-.9 2-2s-.9-2-2-2H12zm1.25-3H13v2h.25a1 1 0 0 0 0-2zM7 17h2.5a1 1 0 0 0 0-2H8.5V14H7zM12 17h2a1 1 0 1 0 0-2h-2z"/></svg>
         </a> -->
         <!-- <a class="btn-icon" href="https://your-website.example" target="_blank" rel="noopener" title="Project Website">
           <svg viewBox="0 0 24 24" aria-hidden="true" fill="currentColor"><path d="M12 2a10 10 0 1 0 .001 20.001A10 10 0 0 0 12 2m0 2c1.2 0 2.32.26 3.33.73-.5.62-1.17 1.66-1.7 3.27H10.4C9.86 6.39 9.2 5.35 8.67 4.73A8 8 0 0 1 12 4m-6.93 8c.12-1.22.52-2.36 1.14-3.37.7.72 1.74 1.2 3.19 1.37A26.3 26.3 0 0 0 9.3 12c0 .36.01.7.03 1.02-1.45.18-2.49.66-3.19 1.38A7.98 7.98 0 0 1 5.07 12m6.85 7.27c-.62-.64-1.33-1.83-1.82-3.77h3.8c-.49 1.94-1.2 3.13-1.98 3.77M8.67 19.27A8 8 0 0 1 4.9 15.6c.5-.58 1.43-1.08 2.92-1.29.37 2.1 1.07 3.62 1.85 4.96M12 20a8 8 0 0 1-3.33-.73c.52-.63 1.18-1.67 1.71-3.27h3.22c.53 1.6 1.2 2.64 1.7 3.27A7.97 7.97 0 0 1 12 20m3.33-.73c.78-1.34 1.48-2.86 1.85-4.96 1.49.21 2.42.71 2.92 1.29a8 8 0 0 1-3.77 3.67M18.93 12c-.12 1.22-.52 2.36-1.14 3.37-.7-.72-1.74-1.2-3.19-1.37.02-.33.03-.67.03-1 0-.35-.01-.69-.03-1.02 1.45-.18 2.49-.66 3.19-1.38.62 1 1.02 2.15 1.14 3.37M15.6 6.73c.37-2.1 1.07-3.62 1.85-4.96A8 8 0 0 1 19.1 8.4c-.5.58-1.43 1.08-2.92 1.29-.37-2.1-1.07-3.62-1.85-4.96Z"/></svg>
         </a> -->
       </nav>
     </div>
     <!-- NEW STUFF ENDS -->
    <h1 id="article-title">Scaling Neuromorphic Systems: A pathway to deploy neuromorphic accelerators</h1>

    <p>
      We present a pragmatic pathway to deploy neuromorphic accelerators in everyday devices, emphasizing
      memory-centric compute, robustness under non-idealities, and a software toolchain that bridges research
      and production [furber2016][merolla2014]. Our approach blends spiking and analog-friendly deep models
      with conventional pipelines to meet latency, energy, and reliability targets under real-world constraints
      [horowitz2014][han2015][paszke2019]. We also contribute an open corpus of tasks representative of edge reality:
      noise, drift, intermittent power, and tight memory budgets [mcdermott2019][dettmers2023].
    </p>

    <section>
      <h2>Introduction</h2>
      <p>
        Modern edge workloadsâ€”from AR glasses to industrial sensorsâ€”demand sub-10&nbsp;ms latency at milliwatt scales
        [horowitz2014][han2015]. Conventional von Neumann architectures struggle with the memory wall; neuromorphic designs
        promise in-memory compute and event-driven sparsity to close this gap [indiveri2015][sebastian2020]. Yet,
        adoption is slowed by brittle algorithms and tooling gaps between ML frameworks and silicon realities
        [paszke2019][dettmers2023]. We argue for a co-design loop where models are trained with hardware noise
        in the loop, and compilers map graph-level constructs to memory-array primitives [sebastian2020][pufall2024].
      </p>
      <p>
        In this work we (i) define a set of benchmarked tasks and perturbations, (ii) propose a mixed-precision,
        memory-first accelerator template, and (iii) release a compiler IR that targets bitline-level operations
        [rumelhart1986][lecun2015][dettmers2023]. Throughout, we quantify trade-offs in accuracy-energy-latency
        under practical limits like array sizes, wire RC, sneak paths, and endurance [ielmini2018][sebastian2020].
      </p>
    </section>

    <section>
      <h2>Background</h2>
      <h3>Compute Near or In Memory</h3>
      <p>
        Moving data dominates energy; a 64-bit DRAM access can cost orders of magnitude more than a MAC
        [horowitz2014][han2015]. Analog and digital CIM arrays reduce movement by aggregating partial sums
        on bitlines or within SRAM/RRAM sub-arrays [khaddam2020][sebastian2020]. Practical deployments need robust
        calibration to handle device variability and temperature drift [ielmini2018][sebastian2020].
      </p>
      <h3>Spiking and Event-Driven Models</h3>
      <p>
        Spiking neural networks (SNNs) exploit sparse temporal coding, but training instabilities and toolchain
        friction remain [zenke2018][neftci2019]. Hybrid approachesâ€”event cameras with conventional backbonesâ€”offer
        a middle ground [gallego2020][gehrig2023].
      </p>
    </section>

    <section>
      <h2>Methods</h2>
      <h3>System Model</h3>
      <p>
        We model energy as $E = E_{mem} + E_{compute} + E_{ctrl}$ with $E_{mem}$ dominated by line capacitances:
        $E_{mem} \\approx \\sum_i C_i V^2$ where $C_i$ aggregates WL/BL parasitics [horowitz2014][khaddam2020].
        We treat non-idealities as stochastic perturbations injected during training [ielmini2018][dettmers2023].
      </p>

      <h3>Compiler IR</h3>
      <p>
        Our IR expresses compute as tiled blocks over arrays with explicit bitline accumulation and
        saturation semantics [pufall2024]. Example lower-level IR snippet:
      </p>
      <pre data-lang="ir"><code>
block gemm_tile(m=64, n=64, k=64) {
  map A[m, k] -> sram0;
  map B[k, n] -> sram1;
  map C[m, n] -> sram2 accum;
  for mm in 0..m step 8:
    for nn in 0..n step 8:
      for kk in 0..k step 8:
        mac8x8(sram0[mm:mm+7, kk:kk+7],
               sram1[kk:kk+7, nn:nn+7]) -> sram2[mm:mm+7, nn:nn+7];
}
      </code></pre>

      <h3>Calibration Routine</h3>
      <pre data-lang="python"><code>
def calibrate_bitlines(arrays, samples=512):
    stats = {}
    for name, arr in arrays.items():
        bl_offsets = measure_offsets(arr, samples)
        bl_scales  = measure_scales(arr, samples)
        stats[name] = (bl_offsets, bl_scales)
    return stats
      </code></pre>

      <h3>Deployment Script</h3>
      <pre data-lang="bash"><code>
#!/usr/bin/env bash
set -euo pipefail
MODEL="$1"
TARGET="/dev/neuromem0"
neuromc compile "$MODEL" --ir out.ir --quant int4 --noise hw
neuromc flash out.ir --target "$TARGET"
neuromc run --target "$TARGET" --dataset edgebench/dev --metrics
      </code></pre>
    </section>

    <section>
      <h2>Datasets and Perturbations</h2>
      <p>
        We introduce <em>EdgeBench</em>, a mixed-modality corpus with audio, vision, IMU, and event streams.
        Each sample includes perturbation descriptors: thermal drift, voltage droop, sensor bias, and memory
        retention stress [mcdermott2019][gallego2020]. For quantization-aware training we follow
        integer-only deployment guidelines and gradient-based rounding [dettmers2023].
      </p>
      <figure>
        <center>
        <img src="../../assets/images/rz0681.webp" alt="EdgeBench tasks and perturbations" /> </center>
        <figcaption>Figure 1: EdgeBench task families and perturbation taxonomy.</figcaption>
      </figure>
    </section>

    <section>
      <h2>Results</h2>
      <h3>Energyâ€“Latencyâ€“Accuracy</h3>
      <p>
        On an int4 SRAM-CIM prototype, we achieve 4.2Ã— energy reduction at iso-accuracy versus a digital SIMD
        baseline, with median latency of 6.8&nbsp;ms on always-on wake-word [han2015][khaddam2020]. Under 10% device
        variability the calibrated system loses &lt;0.6% absolute accuracy [ielmini2018].
      </p>
      <!-- Wrap in a scrolling container if your table is wide -->
      <div class="table-wrap">
        <table>
          <caption>Model Comparison on EdgeBench (dev set)</caption>
      
          <!-- Optional: set relative column widths -->
          <colgroup>
            <col style="width: 34%">
            <col style="width: 22%">
            <col style="width: 22%">
            <col style="width: 22%">
          </colgroup>
      
          <thead>
            <tr>
              <th scope="col">Model</th>
              <th scope="col">Energy (mJ)</th>
              <th scope="col">Latency (ms)</th>
              <th scope="col">Accuracy (%)</th>
            </tr>
          </thead>
      
          <tbody>
            <tr>
              <th scope="row">SIMD-int8</th>
              <td>3.90</td>
              <td>10.7</td>
              <td>92.1</td>
            </tr>
            <tr>
              <th scope="row">CIM-int4</th>
              <td>0.93</td>
              <td>6.8</td>
              <td>92.4</td>
            </tr>
            <tr>
              <th scope="row">CIM-int4 + Cal</th>
              <td>0.95</td>
              <td>7.0</td>
              <td>93.0</td>
            </tr>
          </tbody>
      
          <tfoot>
            <tr>
              <th scope="row">Notes</th>
              <td colspan="3">Mean over 5 runs; Â± std omitted for brevity.</td>
            </tr>
          </tfoot>
        </table>
      </div>


      <h3>Robustness to Non-Idealities</h3>
      <p>
        By injecting measured noise profiles during training, accuracy degradation under combined drift + IR drop
        scenarios improves by 2.1% absolute compared to naÃ¯ve quantization [dettmers2023][ielmini2018].
      </p>
      <table>
        <thead>
          <tr>
            <th scope="col">Variant</th>
            <th scope="col">mJ</th>
            <th scope="col">ms</th>
            <th scope="col">Acc</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <th scope="row">Baseline</th>
            <td>3.90</td>
            <td>10.7</td>
            <td>92.1%</td>
          </tr>
          <tr>
            <th scope="row">Quantized</th>
            <td>1.20</td>
            <td>8.3</td>
            <td>92.0%</td>
          </tr>
        </tbody>
        <caption>Ablation Results</caption>
      </table>

      <div class="table-wrap" style="overflow:auto;">
        <table
          style="
            width:100%;
            border-collapse: collapse;
            border: 1px solid var(--border, #000);
            background: transparent;
          "
        >
          <caption style="caption-side: bottom; text-align: left; padding: 2px 2px; color: var(--muted, #000);">
            Model Comparison on EdgeBench (dev set)
          </caption>
      
          <colgroup>
            <col style="width: 34%">
            <col style="width: 22%">
            <col style="width: 22%">
            <col style="width: 22%">
          </colgroup>
      
          <thead>
            <tr>
              <th style="border:1px solid var(--border); padding:2px; text-align:left;">Model</th>
              <th style="border:1px solid var(--border); padding:2px; text-align:right;">Energy (mJ)</th>
              <th style="border:1px solid var(--border); padding:2px; text-align:right;">Latency (ms)</th>
              <th style="border:1px solid var(--border); padding:2px; text-align:right;">Accuracy (%)</th>
            </tr>
          </thead>
      
          <tbody>
            <tr>
              <th scope="row" style="border:1px solid var(--border, #000); padding:2px; text-align:left;">SIMD-int8</th>
              <td style="border:1px solid var(--border, #000); padding:2px; text-align:right;">3.90</td>
              <td style="border:1px solid var(--border, #000); padding:2px; text-align:right;">10.7</td>
              <td style="border:1px solid var(--border, #000); padding:2px; text-align:right;">92.1</td>
            </tr>
            <tr>
              <th scope="row" style="border:1px solid var(--border, #000); padding:2px; text-align:left;">CIM-int4</th>
              <td style="border:1px solid var(--border, #000); padding:2px; text-align:right;">0.93</td>
              <td style="border:1px solid var(--border, #000); padding:2px; text-align:right;">6.8</td>
              <td style="border:1px solid var(--border, #000); padding:2px; text-align:right;">92.4</td>
            </tr>
            <tr>
              <th scope="row" style="border:1px solid var(--border, --fg); padding:2px; text-align:left;">CIM-int4 + Cal</th>
              <td style="border:1px solid var(--border, --fg); padding:2px; text-align:right;">0.95</td>
              <td style="border:1px solid var(--border, --fg); padding:2px; text-align:right;">7.0</td>
              <td style="border:1px solid var(--border, --fg); padding:2px; text-align:right;">93.0</td>
            </tr>
          </tbody>
      
          <tfoot>
            <tr>
              <th scope="row" style="border:1px solid var(--border, --fg; padding:2px; text-align:left;">Notes</th>
              <td colspan="3" style="border:1px solid var(--border, --fg; padding:2px; text-align:left;">
                Mean over 5 runs; Â± std omitted for brevity.
              </td>
            </tr>
          </tfoot>
        </table>
      </div>


      <h3>Ablations</h3>
      <pre data-lang="text"><code>
Model        | Energy (mJ) | Latency (ms) | Acc (%)
-------------|-------------|--------------|--------
SIMD-int8    |   3.9       |   10.7       | 92.1
CIM-int4     |   0.93      |    6.8       | 92.4
CIM-int4+cal |   0.95      |    7.0       | 93.0
      </code></pre>
    </section>

    <section>
      <h2>Discussion</h2>
      <p>
        While analog CIM promises superior efficiency, maintenance via background calibration is essential
        for stable field performance [ielmini2018][sebastian2020]. Mixed-signal guardrailsâ€”saturation, tiling,
        redundancyâ€”allow graceful degradation without catastrophic failures [khaddam2020]. Tooling remains the
        bottleneck; bridging PyTorch graphs to array-level programs is crucial [paszke2019][pufall2024].
      </p>
    </section>

    <section>
      <h2>Conclusion</h2>
      <p>
        We demonstrate that memory-first accelerators can meet edge constraints with realistic non-idealities,
        provided training, calibration, and compilation are co-designed [sebastian2020][pufall2024]. Future work
        explores adaptive tiling and on-device learning with bounded updates [neftci2019][zenke2018].
      </p>
    </section>

    <section>
      <h2>Acknowledgments</h2>
      <p>
        We thank the open-source communities behind PyTorch [&#8203;Okay] and event-camera tooling [paszke2019][gallego2020] and
        the device-modeling teams whose public datasets enabled robust noise-aware training [ielmini2018].
      </p>
    </section>

    <!-- References (auto-generated, numbers & text) -->
    <section id="references-section">
      <h2>References</h2>
      <ol id="references"></ol>
    </section>

    <!-- Reference catalog for tooltips + generation -->
    <template id="ref-catalog">
      <li data-key="furber2016">Furber, S. (2016). Large-scale neuromorphic computing systems. <i>Journal of Neural Engineering</i>.</li>
      <li data-key="merolla2014">Merolla, P. A., et al. (2014). A million spiking-neuron integrated circuit. <i>Science</i>.</li>
      <li data-key="horowitz2014">Horowitz, M. (2014). 1.1 Computingâ€™s energy problem (and what we can do about it). <i>ISSCC</i>.</li>
      <li data-key="han2015">Han, S., et al. (2015). Learning both weights and connections for efficient neural networks. <i>NIPS</i>.</li>
      <li data-key="paszke2019">Paszke, A., et al. (2019). PyTorch: An imperative style, high-performance deep learning library. <i>NeurIPS</i>.</li>
      <li data-key="mcdermott2019">McDermott, J. H. (2019). Psychophysics of everyday listening. <i>Annual Review of Psychology</i>.</li>
      <li data-key="dettmers2023">Dettmers, T., et al. (2023). QLoRA: Efficient finetuning of quantized LLMs. <i>ICML</i>.</li>
      <li data-key="indiveri2015">Indiveri, G., &amp; Liu, S.-C. (2015). Memory and information processing in neuromorphic systems. <i>Proceedings of the IEEE</i>.</li>
      <li data-key="sebastian2020">Sebastian, A., et al. (2020). Memory devices and applications for in-memory computing. <i>Nature Nanotechnology</i>.</li>
      <li data-key="pufall2024">Pufall, M. R., et al. (2024). A compiler stack for analog in-memory compute. <i>arXiv</i>.</li>
      <li data-key="rumelhart1986">Rumelhart, D. E., et al. (1986). Learning representations by back-propagating errors. <i>Nature</i>.</li>
      <li data-key="lecun2015">LeCun, Y., Bengio, Y., &amp; Hinton, G. (2015). Deep learning. <i>Nature</i>.</li>
      <li data-key="ielmini2018">Ielmini, D., &amp; Wong, H.-S. P. (2018). In-memory computing with resistive switching devices. <i>Nature Electronics</i>.</li>
      <li data-key="khaddam2020">Khaddam-Aljameh, R., et al. (2020). Hermes core: A 14 nm CMOS ADC-integrated computing-in-memory macro. <i>ISSCC</i>.</li>
      <li data-key="zenke2018">Zenke, F., &amp; Ganguli, S. (2018). Superspike: Surrogate gradient learning in SNNs. <i>Neural Computation</i>.</li>
      <li data-key="neftci2019">Neftci, E. O., et al. (2019). Surrogate gradient learning in SNNs. <i>IEEE Signal Processing Magazine</i>.</li>
      <li data-key="gallego2020">Gallego, G., et al. (2020). Event-based vision: A survey. <i>IEEE TPAMI</i>.</li>
      <li data-key="gehrig2023">Gehrig, M., et al. (2023). End-to-end learning of representations for event cameras. <i>CVPR</i>.</li>
    </template>

    <!-- Footer (after references) -->
    <footer id="site-footer" style="margin-top:2rem; padding-top:1rem; border-top:1px solid var(--border); font-size:0.95rem;">
      <p>
        This work is licensed under a Creative Commons Attribution - Non Commercial 4.0 International License.<br> 
        This means you're free to copy, share, and build on this book, but not to sell it in any part or form.
      </p>
      <!-- <p>
        Â© 2025 Hackframe Research â€¢ Contact: <a href="mailto:"></a> â€¢
        Date: October 1, 2025
      </p> -->
    </footer>
  </main>

  <!-- Tooltip used by JS for reference previews -->
  <div id="ref-tooltip" role="tooltip" aria-hidden="true"></div>

</body>
</html>